{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "import pandas as pd\n",
    "import utils\n",
    "from model import generate_model\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from optimizer import Adam,SGD\n",
    "\n",
    "from train_wrapper import train_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.load_config()\n",
    "\n",
    "batch_size = config['dataloader']['batch_size']\n",
    "num_workers = config['dataloader']['num_workers']\n",
    "pin_memory = config['dataloader']['pin_memory'] == 1 \n",
    "gpu_parallel = config['gpus']\n",
    "learning_rate = config['train']['learning_rate']\n",
    "lr_steps = config['train']['lr_steps']\n",
    "epochs = config['train']['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MyGit\\BrainMR_MCI\\dataloader.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dataset['grp'] = (df_dataset['source'].str.replace('OASIS-3','1').str.replace('ADNI','2').apply(pd.to_numeric)*1000\n"
     ]
    }
   ],
   "source": [
    "#데이터셋 분리(Train, validation, test)\n",
    "df_dataset = pd.read_csv(config['PATH_DATASET_CSV'])\n",
    "df_dataset = df_dataset.dropna().reset_index(drop=True)\n",
    "df_oasis = df_dataset[df_dataset['source'] == 'OASIS-3']\n",
    "df_adni = df_dataset[df_dataset['source'] == 'ADNI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data\n",
      " Total: 990    CN : 576 (58.18% of total),   MCI: 263 (26.57% of total),   AD : 151 (15.25% of total)\n",
      "Oasis Data\n",
      " Total: 824    CN : 524 (63.59% of total),   MCI: 183 (22.21% of total),   AD : 117 (14.20% of total)\n",
      "ADNI Data\n",
      " Total: 166    CN : 52 (31.33% of total),   MCI: 80 (48.19% of total),   AD : 34 (20.48% of total)\n"
     ]
    }
   ],
   "source": [
    "#데이터 분포도 확인 \n",
    "raw_ad,raw_cn,raw_mci = pd.get_dummies(df_dataset['group_maxinc']).sum()\n",
    "\n",
    "raw_total = raw_ad + raw_cn + raw_mci\n",
    "print('Raw Data\\n Total: {}    CN : {} ({:.2f}% of total),   MCI: {} ({:.2f}% of total),   AD : {} ({:.2f}% of total)'\n",
    "      .format(raw_total, raw_cn,  100 * raw_cn / raw_total, raw_mci, 100 * raw_mci / raw_total, raw_ad,  100 * raw_ad / raw_total))\n",
    "\n",
    "oasis_ad,oasis_cn,oasis_mci = pd.get_dummies(df_oasis['group_maxinc']).sum()\n",
    "\n",
    "oasis_total = oasis_ad + oasis_cn + oasis_mci\n",
    "print('Oasis Data\\n Total: {}    CN : {} ({:.2f}% of total),   MCI: {} ({:.2f}% of total),   AD : {} ({:.2f}% of total)'\n",
    "      .format(oasis_total, oasis_cn,  100 * oasis_cn / oasis_total, oasis_mci, 100 * oasis_mci / oasis_total, oasis_ad,  100 * oasis_ad / oasis_total))\n",
    "\n",
    "adni_ad,adni_cn,adni_mci = pd.get_dummies(df_adni['group_maxinc']).sum()\n",
    "adni_total = adni_ad + adni_cn + adni_mci\n",
    "print('ADNI Data\\n Total: {}    CN : {} ({:.2f}% of total),   MCI: {} ({:.2f}% of total),   AD : {} ({:.2f}% of total)'\n",
    "      .format(adni_total, adni_cn,  100 * adni_cn / adni_total, adni_mci, 100 * adni_mci / adni_total, adni_ad,  100 * adni_ad / adni_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MyGit\\BrainMR_MCI\\dataloader.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dataset['grp'] = (df_dataset['source'].str.replace('OASIS-3','1').str.replace('ADNI','2').apply(pd.to_numeric)*1000\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,y_train,y_val = dataloader.dataset_split(df_oasis,test_size=0.2,shuffle=True,grp=None,seed=1004)\n",
    "X_test = df_adni.drop(labels='group_maxinc',axis=1)\n",
    "y_test = df_adni['group_maxinc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader :  659\n",
      "val_dataloader :  165\n",
      "test_dataloader :  166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MyGit\\BrainMR_MCI\\pymain\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "import torchio as tio\n",
    "from torch.utils.data import DataLoader,ConcatDataset,WeightedRandomSampler\n",
    "\n",
    "# class 0 : 43200개, class 1 : 4800개\n",
    "class_counts = y_train.value_counts().to_dict() #43200, 4800\n",
    "num_samples = sum(class_counts.values()) # 48000 - 전체 데이터 갯수\n",
    "labels = y_train.to_list()\n",
    "\n",
    "#클래스별 가중치 부여 [48000/43200, 48000/4800] => class 1에 가중치 높게 부여하게 됨\n",
    "class_weights = {i:num_samples / class_counts[i] for i in class_counts.keys()}\n",
    "\n",
    "# 해당 데이터의 label에 해당되는 가중치\n",
    "weights = [class_weights[labels[i]] for i in range(int(num_samples))] #해당 레이블마다의 가중치 비율\n",
    "\n",
    "sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
    "\n",
    "transform = tio.RandomAffine(degrees=(0,0,90)) #이미지 좌우로 랜덤 생성\n",
    "\n",
    "traindata=dataloader.MRIDataset(X_train,y_train)\n",
    "#aug_traindata=dataloader.MRIDataset(X_train,y_train,transform)\n",
    "\n",
    "#train_plus = ConcatDataset([traindata, aug_traindata])\n",
    "\n",
    "valdata=dataloader.MRIDataset(X_val,y_val)\n",
    "testdata=dataloader.MRIDataset(X_test,y_test)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(traindata , batch_size=batch_size, shuffle=False, sampler = sampler\n",
    "                              ,num_workers=num_workers,pin_memory = pin_memory)\n",
    "val_dataloader  = DataLoader(valdata, batch_size=batch_size, shuffle=False\n",
    "                              ,num_workers=num_workers,pin_memory = pin_memory)\n",
    "test_dataloader  = DataLoader(testdata, batch_size=1, shuffle=False)\n",
    "\n",
    "print('train_dataloader : ',len(train_dataloader.dataset))\n",
    "print('val_dataloader : ',len(val_dataloader.dataset))\n",
    "print('test_dataloader : ',len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_name = config['model']['model_name']\n",
    "model_depth = config['model']['model_depth']\n",
    "\n",
    "model, _ = generate_model(model_name=model_name,model_depth = model_depth,n_classes=3,resnet_shortcut='B')\n",
    "model.to(device)\n",
    "\n",
    "if len(gpu_parallel) > 1 and torch.cuda.is_available():\n",
    "    model = nn.DataParallel(model, device_ids = gpu_parallel)\n",
    "    model.to(device)\n",
    "\n",
    "optimizer = Adam(model, learning_rate = learning_rate)\n",
    "criterion_clf = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_messgage(config, messgage='model설명 입력')\n",
    "train_epoch(device,train_dataloader,val_dataloader,test_dataloader,model,criterion_clf,optimizer,config\n",
    "            ,epoch = epochs,age_onoff=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('pymain': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ed8220e6a4d179c92912ea3d00b50a7e26f85dad7a0d0d921d007db80c03c33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
