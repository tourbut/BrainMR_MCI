{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import utils\n",
    "import test\n",
    "\n",
    "from model import generate_model\n",
    "import torch.nn as nn\n",
    "\n",
    "import dataloader\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader :  659\n",
      "val_dataloader :  165\n",
      "test_dataloader :  166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MyGit\\BrainMR_MCI\\dataloader.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dataset['grp'] = (df_dataset['source'].str.replace('OASIS-3','1').str.replace('ADNI','2').apply(pd.to_numeric)*1000\n"
     ]
    }
   ],
   "source": [
    "#데이터셋 분리(Train, validation, test)\n",
    "df_dataset = pd.read_csv(config['PATH_DATASET_CSV'])\n",
    "df_dataset = df_dataset.dropna().reset_index(drop=True)\n",
    "df_oasis = df_dataset[df_dataset['source'] == 'OASIS-3']\n",
    "df_adni = df_dataset[df_dataset['source'] == 'ADNI']\n",
    "X_train,X_val,y_train,y_val = dataloader.dataset_split(df_oasis)\n",
    "X_test = df_adni.drop(labels='group_maxinc',axis=1)\n",
    "y_test = df_adni['group_maxinc']\n",
    "\n",
    "traindata=dataloader.MRIDataset(X_train,y_train)\n",
    "valdata=dataloader.MRIDataset(X_val,y_val)\n",
    "testdata=dataloader.MRIDataset(X_test,y_test)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(traindata, batch_size=1, shuffle=True,num_workers=1)\n",
    "val_dataloader  = DataLoader(valdata, batch_size=1, shuffle=False,num_workers=1)\n",
    "test_dataloader  = DataLoader(testdata, batch_size=1, shuffle=False)\n",
    "\n",
    "print('train_dataloader : ',len(train_dataloader.dataset))\n",
    "print('val_dataloader : ',len(val_dataloader.dataset))\n",
    "print('test_dataloader : ',len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = torch.load('result/resnet50_best.pth', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _ = generate_model(model_name='resnet',model_depth = 50,n_classes=3,resnet_shortcut='B')\n",
    "model.to(device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "log_path = config['log_path']\n",
    "criterion_clf = nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "outputs = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, targets) in enumerate(test_dataloader):\n",
    "        output = model(inputs)\n",
    "        outputs.append(output[0].data.cpu())\n",
    "        labels.append(targets[0].data.cpu())\n",
    "        \n",
    "outputs = torch.stack(outputs)\n",
    "labels =  torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_outputs = []\n",
    "val_labels = []\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, targets) in enumerate(val_dataloader):\n",
    "        output = model(inputs)\n",
    "        val_outputs.append(output[0].data.cpu())\n",
    "        val_labels.append(targets[0].data.cpu())\n",
    "        \n",
    "val_outputs = torch.stack(val_outputs)\n",
    "val_labels =  torch.stack(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(207)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_outputs.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5452)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.functional.classification import multiclass_auroc\n",
    "multiclass_auroc(outputs, labels, num_classes=3, average=\"macro\", thresholds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7389)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_auroc(val_outputs, val_labels, num_classes=3, average=\"macro\", thresholds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[52,  0,  0],\n",
       "        [80,  0,  0],\n",
       "        [34,  0,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "metric = MulticlassConfusionMatrix(num_classes=3)\n",
    "metric(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[108,   0,   0],\n",
       "        [ 25,   0,   0],\n",
       "        [ 32,   0,   0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(val_outputs, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 2, 1, 0, 2, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        2, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0,\n",
       "        0, 2, 2, 1, 0, 2, 0, 0, 0, 2, 1, 1, 1, 2, 2, 0, 0, 0, 2, 0, 1, 2, 1, 0,\n",
       "        2, 0, 1, 1, 2, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 1, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8220, -0.0150, -3.6971],\n",
       "        [ 3.4045, -0.3355, -4.0506],\n",
       "        [ 5.1177, -1.3357, -4.8616],\n",
       "        [ 4.9494, -1.3222, -4.6703],\n",
       "        [ 1.3264,  0.5941, -2.7710],\n",
       "        [ 3.5377, -0.3451, -4.1849],\n",
       "        [ 3.7945, -0.5872, -4.2347],\n",
       "        [ 2.5396,  0.0548, -3.4791],\n",
       "        [ 2.4038,  0.6729, -3.8973],\n",
       "        [ 5.3875, -1.5676, -4.9099],\n",
       "        [ 1.2746, -0.2852, -1.9068],\n",
       "        [ 4.4810, -0.9907, -4.4974],\n",
       "        [ 1.3528, -0.3609, -1.9378],\n",
       "        [ 5.2654, -1.9413, -4.4077],\n",
       "        [ 2.3668,  0.4183, -3.7340],\n",
       "        [ 2.7784,  0.2527, -3.9285],\n",
       "        [ 4.6914, -1.1050, -4.5934],\n",
       "        [ 5.1774, -1.4485, -4.7772],\n",
       "        [ 5.3011, -1.5726, -4.7789],\n",
       "        [ 1.9441,  0.6298, -3.4114],\n",
       "        [ 4.9028, -1.3319, -4.6519],\n",
       "        [ 4.0084, -0.7924, -4.2314],\n",
       "        [ 2.5584,  0.4325, -3.8026],\n",
       "        [ 2.9758,  0.0798, -3.9548],\n",
       "        [ 4.5163, -0.8023, -4.7684],\n",
       "        [ 3.5012, -0.3040, -4.1709],\n",
       "        [ 1.6743,  0.3433, -2.8638],\n",
       "        [ 3.2647, -0.7735, -3.5188],\n",
       "        [ 4.7036, -1.1492, -4.6262],\n",
       "        [ 5.5025, -1.8801, -4.7232],\n",
       "        [ 3.0927,  0.0138, -4.0003],\n",
       "        [ 1.7761, -0.3323, -2.4468],\n",
       "        [ 0.9257,  0.6469, -2.4885],\n",
       "        [ 1.9062,  0.0956, -2.9611],\n",
       "        [ 1.6401,  0.4958, -2.9762],\n",
       "        [ 3.9971, -0.6244, -4.3342],\n",
       "        [ 4.8330, -1.5581, -4.3709],\n",
       "        [ 2.5726,  0.1796, -3.6382],\n",
       "        [ 4.7261, -1.0608, -4.6994],\n",
       "        [ 4.1046, -0.6710, -4.4592],\n",
       "        [ 3.9117, -0.7959, -4.1591],\n",
       "        [ 1.1051,  0.2085, -2.2127],\n",
       "        [ 1.9377,  0.7610, -3.4301],\n",
       "        [ 5.0338, -1.1808, -4.8736],\n",
       "        [ 4.5869, -1.5642, -4.1058],\n",
       "        [ 4.7291, -1.3612, -4.4919],\n",
       "        [ 2.7175, -0.0883, -3.5813],\n",
       "        [ 5.2492, -1.6261, -4.5769],\n",
       "        [ 1.1405,  0.9833, -2.8996],\n",
       "        [ 3.9140, -0.4174, -4.5174],\n",
       "        [ 4.7292, -1.4424, -4.3754],\n",
       "        [ 3.8566, -0.2994, -4.5177],\n",
       "        [ 3.2026, -0.0188, -4.1423],\n",
       "        [ 3.7916, -1.0233, -3.6062],\n",
       "        [ 4.8147, -1.3493, -4.5317],\n",
       "        [ 4.5841, -1.2062, -4.4636],\n",
       "        [ 1.6145,  0.3307, -2.7926],\n",
       "        [ 4.0897, -1.1406, -3.9957],\n",
       "        [ 3.4701, -0.9176, -3.5125],\n",
       "        [ 2.9763,  0.1146, -3.9777],\n",
       "        [ 4.8165, -1.2531, -4.6860],\n",
       "        [ 1.3305, -0.4405, -1.8320],\n",
       "        [ 4.9236, -1.4210, -4.5408],\n",
       "        [ 1.6974,  0.3532, -2.8554],\n",
       "        [ 2.6865, -0.2475, -3.3528],\n",
       "        [ 1.1971,  0.3743, -2.4338],\n",
       "        [ 1.3879,  0.4932, -2.8128],\n",
       "        [ 1.8385,  0.7208, -3.3904],\n",
       "        [ 3.3408, -0.0563, -4.2621],\n",
       "        [ 5.6161, -1.9673, -4.7373],\n",
       "        [ 4.6302, -1.5410, -4.1534],\n",
       "        [ 5.0650, -1.7709, -4.3779],\n",
       "        [ 2.0302, -0.0266, -2.8799],\n",
       "        [ 3.5484, -1.1004, -3.4404],\n",
       "        [ 1.5386,  0.8315, -3.1654],\n",
       "        [ 3.3709, -0.2203, -4.0472],\n",
       "        [ 4.8119, -1.3229, -4.5851],\n",
       "        [ 2.8023, -0.2138, -3.4137],\n",
       "        [ 5.4685, -1.9037, -4.6529],\n",
       "        [ 4.9604, -1.3447, -4.6614],\n",
       "        [ 4.9396, -1.2310, -4.7673],\n",
       "        [ 3.1059, -0.1627, -3.8661],\n",
       "        [ 2.2464,  0.6381, -3.6105],\n",
       "        [ 4.3246, -1.5694, -3.7297],\n",
       "        [ 3.4099, -0.4817, -3.9136],\n",
       "        [ 3.0418, -0.4714, -3.4894],\n",
       "        [ 1.0684, -0.1963, -1.8021],\n",
       "        [ 3.2386, -0.1331, -4.1116],\n",
       "        [ 4.4369, -1.0798, -4.4246],\n",
       "        [ 3.9267, -0.9568, -3.9818],\n",
       "        [ 1.3689,  0.0565, -2.4304],\n",
       "        [ 5.2717, -1.7281, -4.6710],\n",
       "        [ 4.5921, -0.7843, -4.8379],\n",
       "        [ 2.8300, -0.7143, -3.0937],\n",
       "        [ 1.3267,  0.7885, -2.9243],\n",
       "        [ 3.3137, -0.2691, -4.0323],\n",
       "        [ 2.8356, -0.1475, -3.6479],\n",
       "        [ 3.8163, -0.7240, -4.1494],\n",
       "        [ 4.4774, -1.1936, -4.3897],\n",
       "        [ 4.2665, -1.0358, -4.2651],\n",
       "        [ 0.8196,  0.6608, -2.3589],\n",
       "        [ 5.1775, -1.5740, -4.7006],\n",
       "        [ 5.1932, -1.7693, -4.5664],\n",
       "        [ 5.0902, -1.5389, -4.6817],\n",
       "        [ 5.4866, -1.6305, -4.8736],\n",
       "        [ 2.6853,  0.3096, -3.8788],\n",
       "        [ 1.4379,  0.7557, -2.9726],\n",
       "        [ 3.4707, -1.1651, -3.3052],\n",
       "        [ 1.6039,  0.2054, -2.7017],\n",
       "        [ 4.3984, -1.0163, -4.3820],\n",
       "        [ 3.4736, -0.1590, -4.2961],\n",
       "        [ 5.0416, -1.5176, -4.6268],\n",
       "        [ 1.4951,  0.2375, -2.6257],\n",
       "        [ 2.1867,  0.2742, -3.3096],\n",
       "        [ 4.7027, -1.0970, -4.6834],\n",
       "        [ 4.0060, -0.5726, -4.3580],\n",
       "        [ 4.9932, -1.5666, -4.5133],\n",
       "        [ 3.4701, -0.1176, -4.3284],\n",
       "        [ 3.9198, -0.5062, -4.3676],\n",
       "        [ 1.3455,  0.5061, -2.6456],\n",
       "        [ 3.6373, -0.6992, -3.9793],\n",
       "        [ 2.2739,  0.4175, -3.6222],\n",
       "        [ 5.2358, -1.6439, -4.6904],\n",
       "        [ 3.8609, -0.8732, -4.0290],\n",
       "        [ 2.9539, -0.2049, -3.6612],\n",
       "        [ 5.4029, -1.6239, -4.7758],\n",
       "        [ 3.1178,  0.0416, -4.0667],\n",
       "        [ 1.3203, -0.1547, -2.0678],\n",
       "        [ 2.1740, -0.3340, -2.7490],\n",
       "        [ 3.9926, -1.0972, -3.9500],\n",
       "        [ 1.2492,  0.6421, -2.6845],\n",
       "        [ 4.9459, -1.1882, -4.7910],\n",
       "        [ 5.2088, -1.3794, -4.8667],\n",
       "        [ 4.8163, -1.2904, -4.6104],\n",
       "        [ 5.2113, -1.5874, -4.7304],\n",
       "        [ 5.2348, -1.7774, -4.4516],\n",
       "        [ 1.2428, -0.2939, -1.8569],\n",
       "        [ 3.4248, -0.5363, -3.8869],\n",
       "        [ 4.1859, -0.9617, -4.2874],\n",
       "        [ 3.7919, -0.4846, -4.2676],\n",
       "        [ 5.2185, -1.6997, -4.6113],\n",
       "        [ 5.1281, -1.5461, -4.6904],\n",
       "        [ 4.5810, -1.2914, -4.3513],\n",
       "        [ 4.4616, -1.0782, -4.4196],\n",
       "        [ 4.2878, -1.0559, -4.2695],\n",
       "        [ 5.0689, -1.4746, -4.7013],\n",
       "        [ 3.9940, -1.1289, -3.9257],\n",
       "        [ 4.6146, -1.0329, -4.6249],\n",
       "        [ 3.3776, -0.2061, -4.0987],\n",
       "        [ 4.3755, -0.8274, -4.5752],\n",
       "        [ 5.1624, -1.6317, -4.6194],\n",
       "        [ 1.7859,  0.6675, -3.2037],\n",
       "        [ 1.9721, -0.1946, -2.7301],\n",
       "        [ 3.5398, -0.6586, -3.8488],\n",
       "        [ 2.7587, -0.8118, -2.9466],\n",
       "        [ 1.1822,  0.9937, -2.9221],\n",
       "        [ 3.6409, -0.6390, -3.9772],\n",
       "        [ 2.7377,  0.0995, -3.6963],\n",
       "        [ 1.9175,  0.2749, -2.9611],\n",
       "        [ 2.9954,  0.1933, -4.0227],\n",
       "        [ 3.8394, -0.9699, -3.9067],\n",
       "        [ 3.1008, -0.5814, -3.5441],\n",
       "        [ 1.8970,  0.6587, -3.4118],\n",
       "        [ 4.1331, -0.6571, -4.4915],\n",
       "        [ 3.9675, -1.4305, -3.6100]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "target = torch.tensor([2, 1, 0, 0])\n",
    "preds = torch.tensor([[0.96, 0.26, 0.58],\n",
    "                      [0.92, 0.61, 0.17],\n",
    "                      [0.91, 0.09, 0.20],\n",
    "                      [0.95, 0.82, 0.13],\n",
    "])\n",
    "metric = MulticlassConfusionMatrix(num_classes=3)\n",
    "metric(preds, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('pymain': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ed8220e6a4d179c92912ea3d00b50a7e26f85dad7a0d0d921d007db80c03c33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
